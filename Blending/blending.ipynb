{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Main Workflow - Seamless Stable Diffusion CS</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h4><center>Louis LHOTTE | Cl√©ment VERON | Edouard SEGUIER</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Observation and Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> After conducting tests, we realized that some additions were somewhat weird and unrealistic compared to the original image. For instance, adding headphones to a regular photo consistently (across three attempts) resulted in an outlier (very strange style) or half-headphones with poorly matched colors. The idea, therefore, is to add a <b>blending / filtering</b> step to the pipeline so that the headphones integrate better into the photo (aiming for realism and seamless integration).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread('sylvie.jpg')\n",
    "modified_img = cv2.imread('headphones.png')\n",
    "\n",
    "if original_img.shape[:2] != modified_img.shape[:2]:\n",
    "    modified_img = cv2.resize(modified_img, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "diff_mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_diff_mask = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_diff_mask = cv2.threshold(gray_diff_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(binary_diff_mask)\n",
    "center = (x + w // 2, y + h // 2)\n",
    "\n",
    "original_img_lab = cv2.cvtColor(original_img, cv2.COLOR_BGR2LAB)\n",
    "modified_img_lab = cv2.cvtColor(modified_img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "l_orig, a_orig, b_orig = cv2.split(original_img_lab)\n",
    "l_mod, a_mod, b_mod = cv2.split(modified_img_lab)\n",
    "\n",
    "a_mod = cv2.addWeighted(a_mod, 0.9, a_orig, 0.1, 0)\n",
    "b_mod = cv2.addWeighted(b_mod, 0.9, b_orig, 0.1, 0)\n",
    "\n",
    "modified_img_lab = cv2.merge((l_mod, a_mod, b_mod))\n",
    "modified_img = cv2.cvtColor(modified_img_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_mask = cv2.threshold(gray_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Crop the mask to focus only on the bounding box region\n",
    "binary_mask_cropped = np.zeros_like(binary_mask)\n",
    "binary_mask_cropped[y:y+h, x:x+w] = binary_mask[y:y+h, x:x+w]\n",
    "\n",
    "# Smooth transitions but only on modified regions (so that other parts are not modified)\n",
    "feathered_mask = cv2.GaussianBlur(binary_mask_cropped, (15, 15), 0)\n",
    "\n",
    "blended_img = cv2.seamlessClone(modified_img, original_img, feathered_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "cv2.imwrite('blended_output_v1.jpg', blended_img)\n",
    "cv2.imshow('Blended Image', blended_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread('sylvie.jpg')\n",
    "modified_img = cv2.imread('headphones.png')\n",
    "\n",
    "if original_img.shape[:2] != modified_img.shape[:2]:\n",
    "    modified_img = cv2.resize(modified_img, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "diff_mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_diff_mask = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_diff_mask = cv2.threshold(gray_diff_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Morphological operations to refine the mask\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "binary_diff_mask = cv2.erode(binary_diff_mask, kernel, iterations=1)\n",
    "binary_diff_mask = cv2.dilate(binary_diff_mask, kernel, iterations=1)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(binary_diff_mask)\n",
    "center = (x + w // 2, y + h // 2)\n",
    "\n",
    "original_img_lab = cv2.cvtColor(original_img, cv2.COLOR_BGR2LAB)\n",
    "modified_img_lab = cv2.cvtColor(modified_img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "l_orig, a_orig, b_orig = cv2.split(original_img_lab)\n",
    "l_mod, a_mod, b_mod = cv2.split(modified_img_lab)\n",
    "\n",
    "a_mod = cv2.addWeighted(a_mod, 0.9, a_orig, 0.1, 0)\n",
    "b_mod = cv2.addWeighted(b_mod, 0.9, b_orig, 0.1, 0)\n",
    "\n",
    "modified_img_lab = cv2.merge((l_mod, a_mod, b_mod))\n",
    "modified_img = cv2.cvtColor(modified_img_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_mask = cv2.threshold(gray_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "binary_mask_cropped = np.zeros_like(binary_mask)\n",
    "binary_mask_cropped[y:y+h, x:x+w] = binary_mask[y:y+h, x:x+w]\n",
    "\n",
    "# Gaussian blur (feathering)\n",
    "feathered_mask = cv2.GaussianBlur(binary_mask_cropped, (9, 9), 0)\n",
    "\n",
    "blended_img = cv2.seamlessClone(modified_img, original_img, feathered_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "cv2.imwrite('blended_output_v2.jpg', blended_img)\n",
    "cv2.imshow('Blended Image', blended_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Makes the photo more natural so it looks effective to some extent. However, I have some difficulty zoning in the modified region and some unwanted elements gets modified (like the beard unfortunately). Overall colours are also changed slightly but it is linked with the same problem : I am modifying more than I should</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
