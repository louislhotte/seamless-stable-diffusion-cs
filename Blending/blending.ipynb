{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Main Workflow - Seamless Stable Diffusion CS</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h4><center>Louis LHOTTE | Clément VERON | Edouard SEGUIER</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Observation and Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> After conducting tests, we realized that some additions were somewhat weird and unrealistic compared to the original image. For instance, adding headphones to a regular photo consistently (across three attempts) resulted in an outlier (very strange style) or half-headphones with poorly matched colors. The idea, therefore, is to add a <b>blending / filtering</b> step to the pipeline so that the headphones integrate better into the photo (aiming for realism and seamless integration).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread('sylvie.jpg')\n",
    "modified_img = cv2.imread('headphones.png')\n",
    "\n",
    "if original_img.shape[:2] != modified_img.shape[:2]:\n",
    "    modified_img = cv2.resize(modified_img, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "diff_mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_diff_mask = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_diff_mask = cv2.threshold(gray_diff_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "contours, _ = cv2.findContours(binary_diff_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "mask = np.zeros_like(binary_diff_mask)\n",
    "cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "original_region = cv2.bitwise_and(original_img, original_img, mask=mask)\n",
    "modified_region = cv2.bitwise_and(modified_img, modified_img, mask=mask)\n",
    "\n",
    "modified_region_lab = cv2.cvtColor(modified_region, cv2.COLOR_BGR2LAB)\n",
    "l, a, b = cv2.split(modified_region_lab)\n",
    "a = cv2.add(a, 10)\n",
    "b = cv2.add(b, 20)\n",
    "modified_region_lab = cv2.merge((l, a, b))\n",
    "modified_region_colored = cv2.cvtColor(modified_region_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "blended_region = cv2.addWeighted(original_region, 0.4, modified_region_colored, 0.6, 0)\n",
    "\n",
    "final_result = cv2.addWeighted(original_img, 1, blended_region, 1, 0)\n",
    "\n",
    "cv2.imwrite('final_blended_output_v2.jpg', final_result)\n",
    "cv2.imshow('Final Blended Image', final_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread('sylvie.jpg')\n",
    "modified_img = cv2.imread('headphones.png')\n",
    "\n",
    "if original_img.shape[:2] != modified_img.shape[:2]:\n",
    "    modified_img = cv2.resize(modified_img, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "diff_mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_diff_mask = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_diff_mask = cv2.threshold(gray_diff_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Morphological operations to refine the mask\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "binary_diff_mask = cv2.erode(binary_diff_mask, kernel, iterations=1)\n",
    "binary_diff_mask = cv2.dilate(binary_diff_mask, kernel, iterations=1)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(binary_diff_mask)\n",
    "center = (x + w // 2, y + h // 2)\n",
    "\n",
    "original_img_lab = cv2.cvtColor(original_img, cv2.COLOR_BGR2LAB)\n",
    "modified_img_lab = cv2.cvtColor(modified_img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "l_orig, a_orig, b_orig = cv2.split(original_img_lab)\n",
    "l_mod, a_mod, b_mod = cv2.split(modified_img_lab)\n",
    "\n",
    "a_mod = cv2.addWeighted(a_mod, 0.9, a_orig, 0.1, 0)\n",
    "b_mod = cv2.addWeighted(b_mod, 0.9, b_orig, 0.1, 0)\n",
    "\n",
    "modified_img_lab = cv2.merge((l_mod, a_mod, b_mod))\n",
    "modified_img = cv2.cvtColor(modified_img_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_mask = cv2.threshold(gray_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "binary_mask_cropped = np.zeros_like(binary_mask)\n",
    "binary_mask_cropped[y:y+h, x:x+w] = binary_mask[y:y+h, x:x+w]\n",
    "\n",
    "# Gaussian blur (feathering)\n",
    "feathered_mask = cv2.GaussianBlur(binary_mask_cropped, (9, 9), 0)\n",
    "\n",
    "blended_img = cv2.seamlessClone(modified_img, original_img, feathered_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "cv2.imwrite('blended_output_v2.jpg', blended_img)\n",
    "cv2.imshow('Blended Image', blended_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Makes the photo more natural so it looks effective to some extent. However, I have some difficulty zoning in the modified region and some unwanted elements gets modified (like the beard unfortunately). Overall colours are also changed slightly but it is linked with the same problem : I am modifying more than I should</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Space color filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread('sylvie.jpg')\n",
    "modified_img = cv2.imread('headphones.png')\n",
    "\n",
    "if original_img.shape[:2] != modified_img.shape[:2]:\n",
    "    modified_img = cv2.resize(modified_img, (original_img.shape[1], original_img.shape[0]))\n",
    "\n",
    "diff_mask = cv2.absdiff(original_img, modified_img)\n",
    "gray_diff_mask = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "_, binary_diff_mask = cv2.threshold(gray_diff_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "contours, _ = cv2.findContours(binary_diff_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "mask = np.zeros_like(binary_diff_mask)\n",
    "cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "original_region = cv2.bitwise_and(original_img, original_img, mask=mask)\n",
    "modified_region = cv2.bitwise_and(modified_img, modified_img, mask=mask)\n",
    "\n",
    "# Adjust brightness and color with High-Pass Filtering (frequency-based blending)\n",
    "blurred_original = cv2.GaussianBlur(original_region, (21, 21), 0)\n",
    "high_pass_original = cv2.subtract(original_region, blurred_original)\n",
    "\n",
    "modified_blended = cv2.add(modified_region, high_pass_original)\n",
    "\n",
    "# Subtle LAB adjustment to ensure color harmony\n",
    "modified_region_lab = cv2.cvtColor(modified_blended, cv2.COLOR_BGR2LAB)\n",
    "l, a, b = cv2.split(modified_region_lab)\n",
    "a = cv2.add(a, 3)  # Small adjustment in color channels\n",
    "b = cv2.add(b, 5)\n",
    "modified_region_lab_adjusted = cv2.merge((l, a, b))\n",
    "modified_region_final = cv2.cvtColor(modified_region_lab_adjusted, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "blended_region = cv2.addWeighted(original_region, 0.5, modified_region_final, 0.5, 0)\n",
    "final_result = cv2.addWeighted(original_img, 1, blended_region, 1, 0)\n",
    "\n",
    "output_path = 'final_blended_output_v4.jpg'\n",
    "cv2.imwrite(output_path, final_result)\n",
    "cv2.imshow('Final Blended Image', final_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Did not converge yet</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre les deux images: 0.8044\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    return inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "image_path1 = \"sylvie.jpg\"\n",
    "image_path2 = \"blended_output_v2.jpg\"\n",
    "image1 = preprocess_image(image_path1)\n",
    "image2 = preprocess_image(image_path2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features1 = model(image1).last_hidden_state.mean(dim=1)\n",
    "    features2 = model(image2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "similarity = cosine_similarity(features1, features2).item()\n",
    "print(f\"Similarité entre les deux images: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
