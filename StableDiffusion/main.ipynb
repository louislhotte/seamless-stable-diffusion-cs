{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Main Workflow - Seamless Stable Diffusion CS</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h4><center>Louis LHOTTE | Cl√©ment VERON | Edouard SEGUIER</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision & specific region modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> The idea is to improve LEdits++ performance by splitting the prompts on foreground and background, and then reaggregate them further, enabling us to make higher-quality image. If this option was possible, this would open countless possibilities of further development (layers modification, region with clustering modification, etc...)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def segment_foreground_background(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    foreground = cv2.bitwise_and(image, image, mask=mask)\n",
    "    background = cv2.bitwise_and(image, image, mask=cv2.bitwise_not(mask))\n",
    "    return foreground, background\n",
    "\n",
    "foreground, background = segment_foreground_background(\"Landscape.png\")\n",
    "cv2.imwrite(\"foreground.png\", foreground)\n",
    "cv2.imwrite(\"background.png\", background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Aggregation of foreground & background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> This step has the advantage of being highly <b>feasible</b> as it the re-aggregation part is seemingly very fast and as long as there is no major change, the quality of the image is not degraded. It would allow artists and users to best use their creativity without breaking their previous achieved milestones (very common issue in stable diffusion or GANs).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reaggregate_foreground_background(foreground, background):\n",
    "    return cv2.add(foreground, background)\n",
    "\n",
    "foreground = cv2.imread(\"m_foreground.png\")\n",
    "background = cv2.imread(\"background.png\")\n",
    "reaggregated_image = reaggregate_foreground_background(foreground, background)\n",
    "cv2.imwrite(\"reaggregated_image_v3.png\", reaggregated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the image lost quality so we need to upscale it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upscale_image(image_path, scale_factor=2):\n",
    "    image = cv2.imread(image_path)\n",
    "    width = int(image.shape[1] * scale_factor)\n",
    "    height = int(image.shape[0] * scale_factor)\n",
    "    upscaled_image = cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    return upscaled_image\n",
    "\n",
    "upscaled_image = upscale_image(\"reaggregated_image.png\", scale_factor=2)\n",
    "cv2.imwrite(\"upscaled_reaggregated_image.png\", upscaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality infering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enhance_quality_with_modifications(original_path, modified_path):\n",
    "    original = cv2.imread(original_path)\n",
    "    modified = cv2.imread(modified_path)\n",
    "    \n",
    "    original_resized = cv2.resize(original, (modified.shape[1], modified.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    diff_mask = cv2.absdiff(original_resized, modified)\n",
    "    \n",
    "    diff_mask_gray = cv2.cvtColor(diff_mask, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(diff_mask_gray, 30, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    enhanced_image = cv2.addWeighted(modified, 0.7, original_resized, 0.3, 0)\n",
    "    enhanced_image[binary_mask == 0] = original_resized[binary_mask == 0]\n",
    "    \n",
    "    return enhanced_image\n",
    "\n",
    "enhanced_image = enhance_quality_with_modifications(\"original_reaggragated_version.png\", \"upscaled_reaggregated_image.png\")\n",
    "cv2.imwrite(\"enhanced_modified_image.png\", enhanced_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denoise_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 5, 3, 21)\n",
    "    return denoised\n",
    "\n",
    "denoised_image = denoise_image(\"enhanced_modified_image.png\")\n",
    "cv2.imwrite(\"denoised_enhanced_image.png\", denoised_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
