{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Evaluation pipeline</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h4><center>Louis LHOTTE | Cl√©ment VERON | Edouard SEGUIER</center></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_images_with_clip(image_1_path, image_2_path, prompt):\n",
    "    \"\"\"\n",
    "    Evaluate two images based on their relevance to a given text prompt using the CLIP metric.\n",
    "\n",
    "    Args:\n",
    "        image_1_path (str): Path to the first image.\n",
    "        image_2_path (str): Path to the second image.\n",
    "        prompt (str): The text prompt for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing similarity scores for each image.\n",
    "    \"\"\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    image_1 = Image.open(image_1_path).convert(\"RGB\")\n",
    "    image_2 = Image.open(image_2_path).convert(\"RGB\")\n",
    "    inputs = processor(text=[prompt], images=[image_1, image_2], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    image_embeddings = outputs.image_embeds\n",
    "    text_embeddings = outputs.text_embeds\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(image_embeddings, text_embeddings)\n",
    "\n",
    "    result = {\n",
    "        \"image_1_score\": similarity[0].item(),\n",
    "        \"image_2_score\": similarity[1].item(),\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: input_1.jpg, Score: 0.23\n",
      "Output image: output_1.png, Score: 0.29\n",
      "Delta Score: 0.06\n",
      "=====\n",
      "Input image: input_2.jpg, Score: 0.18\n",
      "Output image: output_2.png, Score: 0.28\n",
      "Delta Score: 0.09\n",
      "=====\n",
      "Input image: input_3.jpg, Score: 0.18\n",
      "Output image: output_3.png, Score: 0.23\n",
      "Delta Score: 0.05\n",
      "=====\n",
      "Input image: input_4.png, Score: 0.28\n",
      "Output image: output_4.png, Score: 0.28\n",
      "Delta Score: -0.00\n",
      "=====\n",
      "Input image: input_5.jpg, Score: 0.28\n",
      "Output image: output_5.png, Score: 0.30\n",
      "Delta Score: 0.03\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"input\"\n",
    "    output_dir = \"output\"\n",
    "    prompt_dir = \"Prompts\"\n",
    "\n",
    "    # Get sorted lists of images from both directories\n",
    "    input_images = sorted([f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))])\n",
    "    output_images = sorted([f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))])\n",
    "    prompt_files = sorted([f for f in os.listdir(prompt_dir) if os.path.isfile(os.path.join(prompt_dir, f)) and f.endswith('.txt')])\n",
    "\n",
    "    if len(input_images) != len(output_images):\n",
    "        print(\"Error: Mismatch in the number of input and output images.\")\n",
    "    elif len(input_images) != len(prompt_files):\n",
    "        print(\"Error: Mismatch in the number of images and prompts.\")\n",
    "    else:\n",
    "        for input_image, output_image, prompt_file in zip(input_images, output_images, prompt_files):\n",
    "            input_image_path = os.path.join(input_dir, input_image)\n",
    "            output_image_path = os.path.join(output_dir, output_image)\n",
    "            prompt_path = os.path.join(prompt_dir, prompt_file)\n",
    "\n",
    "            with open(prompt_path, 'r') as file:\n",
    "                prompt = file.read().strip()\n",
    "\n",
    "            scores = evaluate_images_with_clip(input_image_path, output_image_path, prompt)\n",
    "            print(f\"Input image: {input_image}, Score: {scores['image_1_score']:.2f}\")\n",
    "            print(f\"Output image: {output_image}, Score: {scores['image_2_score']:.2f}\")\n",
    "            print(f\"Delta Score: {scores['image_2_score'] - scores['image_1_score']:.2f}\")\n",
    "            print(\"=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">CLIP evaluates the similarity between an image and a given prompt. The delta score can therefore serve as an indicator of the pipeline's \"effectiveness\" or \"performance.\"\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
