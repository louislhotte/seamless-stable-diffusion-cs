{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Stable diffusion</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h4><center>Louis LHOTTE | Clément VERON | Edouard SEGUIER</center></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [02:51<00:00, 2.06MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model_path = \"C:\\\\Users\\\\Louis\\\\OneDrive\\\\The Eggcellent\\\\CentraleSupélec\\\\3A\\\\IA\\\\Headmind Partners IA\\\\ComfyUI\\\\models\\\\clip\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, download_root=clip_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features: tensor([[ 1.5076e-01,  8.2825e-02, -1.4893e-01, -4.7058e-02, -8.1055e-02,\n",
      "         -2.4146e-01,  1.3843e-01, -1.2354e+00, -2.1387e-01,  1.5283e-01,\n",
      "         -1.1444e-01, -1.0620e-01,  4.0710e-02,  2.2961e-01,  8.4900e-02,\n",
      "         -7.1594e-02,  3.1421e-01,  7.5562e-02, -4.1382e-02, -1.2494e-01,\n",
      "          1.4453e-01, -2.9517e-01,  1.4954e-01, -3.1982e-01, -3.7524e-01,\n",
      "          2.0508e-02, -1.6821e-01,  1.2457e-01, -6.0944e-02,  9.8267e-02,\n",
      "         -2.5098e-01, -2.0386e-01,  1.1682e-01, -1.1987e-01,  8.9539e-02,\n",
      "          9.1858e-02, -6.4163e-03, -5.7556e-02, -3.4607e-02, -1.8188e-01,\n",
      "         -2.9004e-01, -1.8237e-01, -2.0276e-01,  2.4951e-01, -1.4539e-01,\n",
      "          1.3916e-01,  2.7069e-02, -1.4636e-01,  1.2073e-01,  3.0298e-01,\n",
      "          7.8186e-02, -1.2610e-01,  2.2308e-02, -8.3618e-02, -7.3853e-03,\n",
      "         -1.7664e-01,  1.4758e-01, -6.0394e-02, -1.6785e-01, -1.7529e-01,\n",
      "          4.5483e-01,  3.1300e-03, -3.7292e-02,  1.5015e-01,  2.1484e-01,\n",
      "          1.3879e-01, -1.6327e-02,  2.4426e-01,  2.0190e-01, -5.4901e-02,\n",
      "          9.0515e-02, -1.3196e-01, -1.0034e-01,  2.9932e-01, -2.0837e-01,\n",
      "         -1.2476e-01, -2.7008e-02,  1.1249e-01, -1.0529e-01, -7.4402e-02,\n",
      "         -2.2385e-02, -3.1592e-01, -2.2925e-01,  5.3833e-02, -3.8818e-02,\n",
      "          5.0537e-02,  1.1658e-01, -3.1934e-01,  2.3413e-01, -3.5706e-02,\n",
      "         -4.2023e-02,  1.4600e-01, -2.1211e+00,  2.2070e-01,  1.3281e-01,\n",
      "         -1.6418e-02, -2.3535e-01,  2.9004e-01,  1.0089e-01, -1.6040e-01,\n",
      "          2.2729e-01,  5.3497e-02,  2.9724e-02,  2.0544e-01,  7.9773e-02,\n",
      "          8.5693e-02, -2.1011e-02, -3.6035e-01, -9.5703e-02, -2.0096e-02,\n",
      "          6.7505e-02,  3.5864e-01,  8.3313e-02,  1.2482e-02,  1.8506e-01,\n",
      "         -8.3923e-02,  1.9849e-01, -7.5378e-02, -2.0142e-01, -8.7463e-02,\n",
      "          1.1334e-01,  1.7822e-01, -5.8685e-02, -3.7048e-02,  6.3354e-02,\n",
      "         -6.0333e-02,  1.7090e-01,  3.0591e-01,  1.9257e-02,  5.6915e-02,\n",
      "          2.3792e-01, -1.7029e-01, -2.8857e-01,  8.3281e+00,  1.0155e-02,\n",
      "          2.1899e-01,  1.0046e-01, -2.6587e-01, -1.4514e-01,  1.8921e-01,\n",
      "         -1.0522e-01, -3.3722e-02, -8.0872e-02, -3.4363e-02, -3.7891e-01,\n",
      "         -1.5137e-01, -3.9136e-01,  2.6855e-01,  2.2620e-01,  2.1057e-01,\n",
      "          1.7786e-01, -2.3755e-01, -2.3828e-01, -2.5635e-01,  2.1619e-01,\n",
      "         -1.3550e-01, -1.5869e-01,  1.2140e-01, -3.7329e-01,  1.4209e-01,\n",
      "         -1.7957e-01, -2.0325e-01, -1.2817e-01, -1.2047e-02, -3.1348e-01,\n",
      "         -4.8889e-02,  3.9038e-01, -3.0688e-01, -3.3203e-02,  8.7830e-02,\n",
      "          5.9143e-02, -1.1948e-02,  1.7334e-01,  3.0225e-01, -1.9336e-01,\n",
      "          3.2898e-02, -1.0034e-01,  2.1130e-01,  4.7461e-01,  2.0251e-01,\n",
      "         -1.5002e-01,  3.0713e-01, -3.4271e-02,  1.6980e-01, -1.4001e-01,\n",
      "          1.4844e-01,  8.0322e-02, -8.1543e-02, -2.2131e-01, -1.1444e-01,\n",
      "         -9.4238e-02, -2.0093e-01,  1.0883e-01,  1.4282e-01,  2.4963e-01,\n",
      "         -2.3926e-01, -5.2917e-02,  2.0691e-02, -1.0938e-01, -4.1351e-02,\n",
      "          1.0175e-01, -4.0161e-01, -1.9608e-02,  4.6051e-02,  1.2741e-02,\n",
      "         -2.3169e-01, -4.4220e-02, -3.3417e-02,  1.7163e-01, -2.8458e-02,\n",
      "          6.9580e-02,  7.6538e-02, -4.7729e-02, -1.9543e-01, -2.8809e-01,\n",
      "          1.0724e-01, -3.7256e-01, -4.8767e-02,  4.4434e-01,  9.1248e-02,\n",
      "          1.0327e-01, -1.6931e-01,  9.8694e-02, -3.7323e-02,  1.3342e-01,\n",
      "         -3.9581e-02, -1.2158e-01,  3.1738e-01,  6.6338e-03,  5.4871e-02,\n",
      "          1.3904e-01,  2.0081e-01, -1.9897e-01,  2.1729e-01,  3.2935e-01,\n",
      "          6.0211e-02, -1.2268e-01, -2.3972e-02, -1.6602e-01,  1.6870e-01,\n",
      "         -2.0911e-01,  1.2177e-01,  1.4807e-01, -3.7329e-01, -6.5674e-02,\n",
      "          4.1504e-01,  8.0750e-02, -3.4241e-02,  1.0834e-01, -1.4758e-01,\n",
      "         -4.4952e-02,  2.2980e-02, -1.8347e-01, -5.4871e-02,  2.9968e-02,\n",
      "          2.2308e-02,  1.1182e-01, -4.7272e-02,  1.1847e-01,  2.2412e-01,\n",
      "          1.3293e-01, -1.8909e-01, -1.7014e-02,  1.2384e-01, -1.1566e-01,\n",
      "         -6.4575e-02, -2.1265e-01, -1.6504e-01, -1.2500e-01,  8.7891e-02,\n",
      "         -2.6929e-01,  1.9638e-02, -2.8061e-02,  8.0444e-02,  7.5867e-02,\n",
      "         -3.8147e-02, -6.3110e-02, -3.2227e-02, -5.5542e-02,  2.0654e-01,\n",
      "          4.6289e-01, -1.4572e-02,  1.6931e-01, -2.1545e-01,  4.3518e-02,\n",
      "         -7.4402e-02,  8.5632e-02, -5.0995e-02,  4.1779e-02,  5.0903e-02,\n",
      "          2.3242e-01, -7.5256e-02, -1.0675e-01,  1.2170e-01, -6.6040e-02,\n",
      "          4.2648e-03, -3.1470e-01, -1.5100e-01,  2.5293e-01,  1.9385e-01,\n",
      "          3.2776e-02, -2.9663e-01,  2.5024e-01,  8.2520e-02, -1.9058e-02,\n",
      "          4.8633e-01, -4.6265e-02, -2.0618e-01,  1.8762e-01,  4.5227e-02,\n",
      "          1.7065e-01, -2.1338e-01,  8.3203e+00, -1.6016e-01, -1.3847e-02,\n",
      "         -3.7170e-02,  5.8899e-02, -4.3121e-02, -1.4392e-01,  5.6946e-02,\n",
      "          3.8452e-01,  6.2256e-01, -8.4900e-02,  1.9275e-01, -2.7808e-01,\n",
      "          1.1761e-01,  5.8105e-02, -1.5430e-01,  6.5552e-02, -3.2227e+00,\n",
      "         -1.4862e-02,  5.9570e-02, -1.0211e-01,  2.3633e-01,  1.6394e-01,\n",
      "         -2.3450e-01,  2.9492e-01,  3.0664e-01,  2.9663e-01, -9.0942e-02,\n",
      "          6.2195e-02,  6.8970e-02,  1.1055e-02, -1.5649e-01,  8.9417e-02,\n",
      "          2.3911e-02, -5.2452e-03, -1.0480e-01,  1.8213e-01,  8.7341e-02,\n",
      "         -3.6469e-02,  2.6810e-02,  5.7861e-02,  2.8711e-01,  5.7220e-03,\n",
      "          3.6060e-01,  2.2229e-01,  2.3889e-01,  7.6721e-02,  1.1871e-02,\n",
      "         -1.7993e-01,  2.2668e-01,  6.1963e-01,  3.5864e-01, -3.3521e-01,\n",
      "          1.2524e-01, -9.9304e-02,  6.5491e-02, -9.2529e-02,  4.2496e-03,\n",
      "         -3.4424e-02,  3.2959e-01, -1.8066e-01,  3.3008e-01, -3.6353e-01,\n",
      "         -8.5449e-02,  4.7363e-02, -8.8318e-02, -4.1064e-01, -5.0354e-02,\n",
      "          2.0129e-01, -4.2603e-01, -3.7842e-02,  2.7197e-01,  1.5710e-01,\n",
      "          1.5297e-03, -2.8198e-01, -1.0052e-01,  2.2812e-03,  6.6589e-02,\n",
      "         -2.4243e-01, -3.7646e-01, -7.5867e-02,  1.9446e-01, -2.1887e-01,\n",
      "          1.1926e-01, -3.5254e-01, -1.3290e-02, -9.1095e-03,  1.9092e-01,\n",
      "          6.2141e-03, -8.4717e-02,  4.8981e-02, -8.0811e-02,  1.1609e-01,\n",
      "          2.2571e-01, -2.3819e-02,  1.4795e-01,  2.2412e-01,  8.9417e-02,\n",
      "         -1.6895e-01,  2.3669e-01,  9.0942e-02, -9.4238e-02,  2.1631e-01,\n",
      "          1.1742e-02,  1.8896e-01,  8.4595e-02,  4.7836e-03,  3.2251e-01,\n",
      "         -9.0149e-02, -1.8201e-01, -3.9124e-02, -2.1194e-02, -1.2073e-01,\n",
      "          4.2139e-01, -1.5205e-02,  7.7026e-02, -2.7759e-01,  7.6782e-02,\n",
      "          2.7344e-01, -1.8555e-02, -2.7466e-01, -2.2314e-01, -6.6895e-02,\n",
      "         -1.1725e-01, -2.0496e-01, -2.8564e-01,  8.6121e-02, -7.1411e-02,\n",
      "          1.1011e-01,  1.4282e-02, -3.6499e-01, -1.7163e-01, -4.7791e-02,\n",
      "          6.2408e-02, -1.7993e-01, -1.2634e-01,  4.5166e-02, -2.0886e-01,\n",
      "         -4.5166e-02,  1.3232e-01, -2.9834e-01,  4.2114e-01,  2.2839e-01,\n",
      "         -7.0923e-02,  2.4689e-02,  2.4512e-01, -1.1505e-01,  4.8981e-02,\n",
      "          2.0325e-02, -2.0300e-01, -2.1313e-01,  6.7444e-02, -9.1324e-03,\n",
      "         -1.0529e-01,  5.9052e-02,  2.5684e-01, -2.1167e-01,  2.1777e-01,\n",
      "          1.2732e-04,  7.8979e-02,  2.8809e-01, -2.8223e-01,  2.3828e-01,\n",
      "          7.5317e-02,  2.0215e-01,  2.3767e-01, -1.1310e-01,  2.5708e-01,\n",
      "          3.3630e-02, -1.1416e+00, -6.7993e-02,  2.0068e-01, -7.1350e-02,\n",
      "         -1.1725e-01, -4.6661e-02,  2.5293e-01,  2.6636e-01,  3.5706e-02,\n",
      "         -2.2449e-01, -1.6736e-01,  1.6040e-01, -3.4399e-01,  8.0414e-03,\n",
      "         -5.5206e-02,  1.4795e-01, -2.5537e-01,  3.3374e-01,  3.1934e-01,\n",
      "         -1.8860e-01, -2.3303e-01,  1.9226e-01,  1.7822e-01,  3.5370e-02,\n",
      "          4.0039e-02, -3.9581e-02,  2.1225e-02, -6.6772e-02, -3.3081e-02,\n",
      "         -2.2864e-01,  3.2520e-01]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "text = [\"Hello, this is a test!\"]\n",
    "tokens = clip.tokenize(text).to(device)\n",
    "\n",
    "# Passage dans le modèle\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(tokens)\n",
    "\n",
    "print(\"Text features:\", text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m clip_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLouis\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mThe Eggcellent\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCentraleSupélec\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m3A\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mIA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHeadmind Partners IA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mComfyUI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m n_path \u001b[38;5;241m=\u001b[39m clip_model_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mViT-B-32-pytorch.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:764\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "clip_model_path = \"C:\\\\Users\\\\Louis\\\\OneDrive\\\\The Eggcellent\\\\CentraleSupélec\\\\3A\\\\IA\\\\Headmind Partners IA\\\\ComfyUI\\\\models\\\\clip\"\n",
    "n_path = clip_model_path + \"\\\\ViT-B-32-pytorch.pth\"\n",
    "# Load the TorchScript model\n",
    "model = torch.jit.load(clip_model_path + '\\\\ViT-B-32.pt')\n",
    "\n",
    "# Extract and save the state dict\n",
    "torch.save(model.state_dict(), n_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
